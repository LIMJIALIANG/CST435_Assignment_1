# Single Container vs Multiple Containers Comparison

## Test Results (October 25, 2025)

**Dataset**: sample_text.txt (1206 characters, 156 words)

### Performance Comparison

| Configuration | Service 1 (Word Count) | Service 2 (Alphabetical Sort) | Service 3 (Word Length) | **Total Time** |
|--------------|----------------------|------------------------------|----------------------|--------------|
| **1 Server** | 0.0049s | 0.0033s | 0.0059s | **0.0140s** |
| **3 Servers** | 0.0121s | 0.0078s | 0.0209s | **0.0408s** |
| **Difference** | +147% slower | +136% slower | +254% slower | **+191% slower** |

### Key Findings

#### üèÜ Single Server is FASTER (2.9x)
- **Winner**: Single server at 0.0140s
- **3 Servers**: 0.0408s
- **Overhead**: Network communication + container coordination adds significant overhead

#### Why Single Server Wins for Small Data:

1. **No Network Overhead**
   - Single server: All processing happens locally in memory
   - 3 Servers: Each request travels over Docker network (even if virtual)

2. **No Data Splitting/Merging Overhead**
   - Single server: Processes entire text at once
   - 3 Servers: Must split text into 3 chunks, process separately, then merge results

3. **Simpler Coordination**
   - Single server: Direct function calls
   - 3 Servers: gRPC serialization + deserialization for each chunk

4. **Small Dataset Size**
   - With only 156 words, the overhead of distribution exceeds the benefit of parallelization

---

## When to Use Each Approach

### ‚úÖ Use Single Server When:
- Dataset is small (< 1 MB)
- Response time is critical
- Simple deployment is preferred
- Development/testing environment
- Cost optimization is important

### ‚úÖ Use Multiple Servers When:
- Dataset is large (> 100 MB)
- Scalability is required
- Fault tolerance is needed (one server can fail, others continue)
- Multiple physical machines are available
- Production environment with high load
- Processing time > network overhead

---

## Detailed Breakdown by Service

### Service 1: Word Count (MapReduce)
```
Single Server:  0.0049s
3 Servers:      0.0121s
Overhead:       +147%
```
**Analysis**: Map phase benefits from parallel processing, but reduce phase requires aggregating results from all servers, adding overhead.

### Service 2: Alphabetical Sort
```
Single Server:  0.0033s
3 Servers:      0.0078s
Overhead:       +136%
```
**Analysis**: Merge sort on distributed chunks requires final merge operation across all sorted sublists, doubling the time.

### Service 3: Word Length Analysis
```
Single Server:  0.0059s
3 Servers:      0.0209s
Overhead:       +254%
```
**Analysis**: Statistical aggregation requires coordination across all servers to compute min/max/average, adding significant overhead.

---

## Architecture Diagrams

### Single Server Architecture
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           Single Container              ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ       Client Process              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Connects to 1 server           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Sends entire text              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Receives complete result       ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ               ‚îÇ                         ‚îÇ
‚îÇ               ‚ñº                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ       Server Process              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - All 3 services available       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Processes entire dataset       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Returns complete result        ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Multiple Servers Architecture
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   Client Container                   ‚îÇ
‚îÇ  - Connects to 3 servers                            ‚îÇ
‚îÇ  - Splits text into 3 chunks                        ‚îÇ
‚îÇ  - Distributes chunks across servers                ‚îÇ
‚îÇ  - Aggregates results                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                ‚îÇ                ‚îÇ
         ‚ñº                ‚ñº                ‚ñº
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ Server 1 ‚îÇ     ‚îÇ Server 2 ‚îÇ     ‚îÇ Server 3 ‚îÇ
   ‚îÇ  Port    ‚îÇ     ‚îÇ  Port    ‚îÇ     ‚îÇ  Port    ‚îÇ
   ‚îÇ  50051   ‚îÇ     ‚îÇ  50052   ‚îÇ     ‚îÇ  50053   ‚îÇ
   ‚îÇ          ‚îÇ     ‚îÇ          ‚îÇ     ‚îÇ          ‚îÇ
   ‚îÇ Process  ‚îÇ     ‚îÇ Process  ‚îÇ     ‚îÇ Process  ‚îÇ
   ‚îÇ chunk 1  ‚îÇ     ‚îÇ chunk 2  ‚îÇ     ‚îÇ chunk 3  ‚îÇ
   ‚îÇ (33% of  ‚îÇ     ‚îÇ (33% of  ‚îÇ     ‚îÇ (34% of  ‚îÇ
   ‚îÇ  text)   ‚îÇ     ‚îÇ  text)   ‚îÇ     ‚îÇ  text)   ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## How to Run Each Configuration

### Single Server (Faster for small data)
```bash
# Start 1 server
cd docker
docker-compose up -d grpc-server-1

# Run single-server client
cd ../grpc_implementation
python client_single_server.py
```

### Multiple Servers (Better for large data)
```bash
# Start 3 servers
cd docker
docker-compose up -d grpc-server-1 grpc-server-2 grpc-server-3

# Run multi-server client
cd ../grpc_implementation
python client_multi_servers.py
```

---

## Recommendation

**For this dataset (1206 characters):**
- ‚úÖ **Use single server** - 2.9x faster
- ‚ùå **Avoid 3 servers** - overhead > benefit

**For larger datasets (e.g., 1 GB text file):**
- ‚ùå **Avoid single server** - will be bottlenecked
- ‚úÖ **Use 3 servers** - parallel processing will win

**Breakeven Point**: Approximately 10-50 MB of text data
- Below this: single server is faster
- Above this: multiple servers become faster

---

## Conclusion

This demonstrates the classic **distributed computing tradeoff**:
- **Small data**: Centralized processing wins (lower overhead)
- **Large data**: Distributed processing wins (parallelization benefit > overhead)

The 2.9x performance difference shows that containerization and network communication add significant overhead that only pays off with larger datasets.
